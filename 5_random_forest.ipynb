{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 暂时不用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>comment_max</th>\n",
       "      <th>comment_mean</th>\n",
       "      <th>comment_min</th>\n",
       "      <th>comments</th>\n",
       "      <th>emoji</th>\n",
       "      <th>english</th>\n",
       "      <th>forward_max</th>\n",
       "      <th>forward_mean</th>\n",
       "      <th>forward_min</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>time</th>\n",
       "      <th>time_hour</th>\n",
       "      <th>time_weekday</th>\n",
       "      <th>time_weekend</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>video</th>\n",
       "      <th>vote</th>\n",
       "      <th>weibo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.516793</td>\n",
       "      <td>2015-02-23 17:41:29</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d38e9bed5d98110dc2489d0d1cac3c2a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7d45833d9865727a88b960b0603c19f6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.473400</td>\n",
       "      <td>2015-03-14 21:22:57</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>d38e9bed5d98110dc2489d0d1cac3c2a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4fedf3888b1e16592f0e0bdc8b393845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.328308</td>\n",
       "      <td>2015-06-18 16:25:36</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d38e9bed5d98110dc2489d0d1cac3c2a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91be0b8612265aae32725cd4fa80b222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.516793</td>\n",
       "      <td>2015-02-23 17:35:31</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d38e9bed5d98110dc2489d0d1cac3c2a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bd2af99ecf1298f5539f0ddfcdd3ed64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.516793</td>\n",
       "      <td>2015-02-23 17:30:13</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d38e9bed5d98110dc2489d0d1cac3c2a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182078c5a409834f2128b3c9c2c289c3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book  comment_max  comment_mean  comment_min  comments  emoji  english  \\\n",
       "0     0         48.0      0.283186          0.0         0      0        0   \n",
       "1     0         48.0      0.283186          0.0         0      0        0   \n",
       "2     0         48.0      0.283186          0.0         0      0        0   \n",
       "3     0         48.0      0.283186          0.0         0      0        0   \n",
       "4     0         48.0      0.283186          0.0         0      0        0   \n",
       "\n",
       "   forward_max  forward_mean  forward_min  ...     tfidf                 time  \\\n",
       "0        114.0      0.830927          0.0  ...  2.516793  2015-02-23 17:41:29   \n",
       "1        114.0      0.830927          0.0  ...  2.473400  2015-03-14 21:22:57   \n",
       "2        114.0      0.830927          0.0  ...  1.328308  2015-06-18 16:25:36   \n",
       "3        114.0      0.830927          0.0  ...  2.516793  2015-02-23 17:35:31   \n",
       "4        114.0      0.830927          0.0  ...  2.516793  2015-02-23 17:30:13   \n",
       "\n",
       "   time_hour  time_weekday  time_weekend  title  \\\n",
       "0         17             1             0      0   \n",
       "1         21             6             1      0   \n",
       "2         16             4             0      0   \n",
       "3         17             1             0      0   \n",
       "4         17             1             0      0   \n",
       "\n",
       "                            user_id  video  vote  \\\n",
       "0  d38e9bed5d98110dc2489d0d1cac3c2a      0     0   \n",
       "1  d38e9bed5d98110dc2489d0d1cac3c2a      0     0   \n",
       "2  d38e9bed5d98110dc2489d0d1cac3c2a      0     0   \n",
       "3  d38e9bed5d98110dc2489d0d1cac3c2a      0     0   \n",
       "4  d38e9bed5d98110dc2489d0d1cac3c2a      0     0   \n",
       "\n",
       "                           weibo_id  \n",
       "0  7d45833d9865727a88b960b0603c19f6  \n",
       "1  4fedf3888b1e16592f0e0bdc8b393845  \n",
       "2  91be0b8612265aae32725cd4fa80b222  \n",
       "3  bd2af99ecf1298f5539f0ddfcdd3ed64  \n",
       "4  182078c5a409834f2128b3c9c2c289c3  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "# from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "data_train=pd.read_csv('data_train.txt',index_col=[0],header=0)\n",
    "data_valid=pd.read_csv('data_valid.txt',index_col=[0],header=0)\n",
    "# 完整的\n",
    "data_train = data_train.append(data_valid)\n",
    "\n",
    "features_list = list(data_train.columns)[3:7] + list(data_train.columns)[8:]\n",
    "train_subset=data_train.loc[:,features_list]\n",
    "valid_subset=data_valid.loc[:,features_list]\n",
    "train_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(data_train, data_test, estimator, depth, leaf_num, split_num, init=False):\n",
    "    if init:\n",
    "        data_test['repost_hat']=0\n",
    "        data_test['comments_hat']=0\n",
    "        data_test['likes_hat']=0\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #噪音数据预测值均为0,因此仅处理非噪音数据\n",
    "    train=data_train[data_train['logit']==0]\n",
    "    test=data_test[data_test['logit']==0]\n",
    "    \n",
    "    #分别对转发，评论和点赞建立三个森林\n",
    "    forest_repost = RandomForestRegressor(n_estimators = estimator, oob_score = True, n_jobs = -1,random_state =50,\n",
    "                                max_features = \"auto\", max_depth = depth, min_samples_leaf = leaf_num, min_samples_split = split_num) # criterion defalut by 'mse'\n",
    "    forest_comments= RandomForestRegressor(n_estimators = estimator, oob_score = True, n_jobs = -1,random_state =50,\n",
    "                                max_features = \"auto\", max_depth = depth, min_samples_leaf = leaf_num, min_samples_split = split_num)\n",
    "    forest_likes = RandomForestRegressor(n_estimators = estimator, oob_score = True, n_jobs = -1,random_state =50,\n",
    "                                max_features = \"auto\", max_depth = depth, min_samples_leaf = leaf_num, min_samples_split = split_num)\n",
    "    #forest_repost = RandomForestRegressor(n_estimators = estimator, oob_score = True, n_jobs = -1\n",
    "    #                            max_features = \"auto\") # criterion defalut by 'mse'\n",
    "    #forest_comments= RandomForestRegressor(n_estimators = estimator, oob_score = True, n_jobs = -1,random_state =42,\n",
    "    #                            max_features = \"auto\")\n",
    "    #forest_likes = RandomForestRegressor(n_estimators = estimator, oob_score = True, n_jobs = -1,random_state =42,\n",
    "    #                            max_features = \"auto\")\n",
    "    \n",
    "    #拟合三个森林\n",
    "    regressor_train=train.drop(['repost','comments','likes','logit'],axis=1, inplace=False)\n",
    "    repost_train=train.loc[:,['repost']]\n",
    "    comments_train=train.loc[:,['comments']]\n",
    "    likes_train=train.loc[:,['likes']]\n",
    "    predict_repost=forest_repost.fit(regressor_train,repost_train.values.ravel()) # shape warning remove\n",
    "    predict_comments=forest_comments.fit(regressor_train,comments_train.values.ravel())\n",
    "    predict_likes=forest_likes.fit(regressor_train,likes_train.values.ravel())\n",
    "    print(predict_repost.oob_score_, predict_comments.oob_score_, predict_likes.oob_score_)\n",
    "    \n",
    "    #预测测试集的数据\n",
    "    regressor_test=test.drop(['repost','comments','likes','logit','repost_hat','comments_hat','likes_hat'],axis=1, inplace=False)\n",
    "    repost_hat=np.round(predict_repost.predict(regressor_test),0)  #round函数只是返回四舍五入值，是浮点类型\n",
    "    comments_hat=np.round(predict_comments.predict(regressor_test),0)\n",
    "    likes_hat=np.round(predict_likes.predict(regressor_test),0)\n",
    "    \n",
    "    #将预测值赋值并设置为整数值\n",
    "    data_test['repost_hat'][data_test['logit']==0]=repost_hat\n",
    "    data_test['comments_hat'][data_test['logit']==0]=comments_hat\n",
    "    data_test['likes_hat'][data_test['logit']==0]=likes_hat\n",
    "    data_test['repost_hat']=data_test['repost_hat'].apply(lambda x:int(x))\n",
    "    data_test['comments_hat']=data_test['comments_hat'].apply(lambda x:int(x))\n",
    "    data_test['likes_hat']=data_test['likes_hat'].apply(lambda x:int(x))\n",
    "    return data_test    \n",
    "\n",
    "\n",
    "def precision(data):\n",
    "    data['deviation_repost']=list(map(lambda x, y: abs(x-y)/(y+5), data['repost_hat'],data['repost']))\n",
    "    #print (data['deviation_repost'])\n",
    "    data['deviation_likes']=list(map(lambda x, y: abs(x-y)/(y+3), data['likes_hat'],data['likes']))\n",
    "    #print (data['deviation_likes'])\n",
    "    data['deviation_comments']=list(map(lambda x, y: abs(x-y)/(y+3), data['comments_hat'],data['comments']))\n",
    "    #print (data['deviation_comments'])\n",
    "    data['lcf_sum']=data['repost']+data['likes']+data['comments']\n",
    "    #print (data['lcf_sum'])\n",
    "    data['lcf_sum']=data['lcf_sum'].apply(lambda x: 100 if x>100 else x)\n",
    "    data['precision_1_-0.8']=1-0.5*data['deviation_repost']-0.25*data['deviation_likes']-0.25*data['deviation_comments']-0.8\n",
    "    #print (data['precision_1_-0.8'])\n",
    "    data.loc[data['precision_1_-0.8']<=0,'sgn']=0\n",
    "    data.loc[data['precision_1_-0.8']>0,'sgn']=1\n",
    "    #print (data['sgn'])\n",
    "    precision_=sum((data['lcf_sum']+1)*data['sgn'])/sum(data['lcf_sum']+1)\n",
    "    return precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29030153173463363"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_subset=random_forest(train_subset, valid_subset, 500, 50, 10, 10) # estimator, depth, leaf_num, split_num, \n",
    "precision(valid_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14790829919259862, 0.15063227028339232, 0.37990754777761826)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### constants\n",
    "estimators = 1000\n",
    "oob = True\n",
    "random_num = 42\n",
    "jobs = -1\n",
    "depth = 50\n",
    "leaf_num = 10\n",
    "split_num = 50\n",
    "\n",
    "def precision(data):\n",
    "    data['deviation_repost']=list(map(lambda x, y: abs(x-y)/(y+5), data['repost_hat'],data['repost']))\n",
    "    #print (data['deviation_repost'])\n",
    "    data['deviation_likes']=list(map(lambda x, y: abs(x-y)/(y+3), data['likes_hat'],data['likes']))\n",
    "    #print (data['deviation_likes'])\n",
    "    data['deviation_comments']=list(map(lambda x, y: abs(x-y)/(y+3), data['comments_hat'],data['comments']))\n",
    "    #print (data['deviation_comments'])\n",
    "    data['lcf_sum']=data['repost']+data['likes']+data['comments']\n",
    "    #print (data['lcf_sum'])\n",
    "    data['lcf_sum']=data['lcf_sum'].apply(lambda x: 100 if x>100 else x)\n",
    "    data['precision_1_-0.8']=1-0.5*data['deviation_repost']-0.25*data['deviation_likes']-0.25*data['deviation_comments']-0.8\n",
    "    #print (data['precision_1_-0.8'])\n",
    "    data.loc[data['precision_1_-0.8']<=0,'sgn']=0\n",
    "    data.loc[data['precision_1_-0.8']>0,'sgn']=1\n",
    "    #print (data['sgn'])\n",
    "    precision_=sum((data['lcf_sum']+1)*data['sgn'])/sum(data['lcf_sum']+1)\n",
    "    return precision_\n",
    "\n",
    "\n",
    "x=data_train[features_list]\n",
    "y=data_train['repost']\n",
    "model_repost =  RandomForestRegressor( n_estimators = estimators , oob_score = oob, random_state = random_num,  n_jobs = jobs,\n",
    "                                max_features = \"auto\", max_depth = depth, min_samples_leaf = leaf_num, min_samples_split = split_num)\n",
    "model_repost.fit(x,y)\n",
    "#model_repost.score\n",
    "\n",
    "x=data_train[features_list]\n",
    "y=data_train['comments']\n",
    "model_comments =  RandomForestRegressor( n_estimators = estimators , oob_score = oob, random_state = random_num,  n_jobs = jobs,\n",
    "                                max_features = \"auto\", max_depth = depth, min_samples_leaf = leaf_num, min_samples_split = split_num)\n",
    "model_comments.fit(x,y)\n",
    "#model_comments.score\n",
    "\n",
    "x=data_train[features_list]\n",
    "y=data_train['likes']\n",
    "model_likes =  RandomForestRegressor( n_estimators = estimators , oob_score = oob, random_state = random_num,  n_jobs = jobs,\n",
    "                                max_features = \"auto\", max_depth = depth, min_samples_leaf = leaf_num, min_samples_split = split_num)\n",
    "model_likes.fit(x,y)\n",
    "#model_likes.score\n",
    "\n",
    "model_repost.oob_score_, model_comments.oob_score_, model_likes.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD THE MODEL FROM THE DISK\n",
    "model_repost = joblib.load('./model/model_repost.sav')\n",
    "model_comments = joblib.load('./model/model_comments.sav')\n",
    "model_likes = joblib.load('./model/model_likes.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林、逻辑回归、均值复合预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "data_train=pd.read_csv('data_train.txt',index_col=[0],header=0)\n",
    "data_valid=pd.read_csv('data_valid.txt',index_col=[0],header=0)\n",
    "#data_train=data_train.append(data_valid)\n",
    "\n",
    "features_list = ['tfidf', 'time_weekday', 'time_weekend', 'time_hour', 'panduan',\n",
    "                   'length_all', 'length_chinese', 'english', 'non_ch', 'sharing', 'auto',\n",
    "                   'interaction', 'book', 'mention', 'vote', 'lottery', 'emoji', 'video',\n",
    "                   'http', 'stock', 'app', 'title', 'ad', 'hotwords', 'keywords',\n",
    "                   'is_noise', 'number_in_train', 'forward_max', 'comment_max', 'like_max',\n",
    "                   'forward_min', 'comment_min', 'like_min', 'forward_mean',\n",
    "                   'comment_mean', 'like_mean', 'forward_more_ave_pr',\n",
    "                   'comment_more_ave_pr', 'like_more_ave_pr', 'max_f/l', 'max_c/l',\n",
    "                   'min_f/l', 'min_c/l', 'mean_f/l', 'mean_c/l', 'night', 'logit']\n",
    "train_subset=data_train.loc[:,features_list]\n",
    "valid_subset=data_valid.loc[:,features_list]\n",
    "\n",
    "noise_cancellation = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2408603143197938"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_test = data_valid\n",
    "data_valid_test['repost_hat']   =0\n",
    "data_valid_test['comments_hat'] =0\n",
    "data_valid_test['likes_hat']    =0\n",
    "if noise_cancellation == True:\n",
    "    #噪音数据预测值均为0,因此仅处理非噪音数据[['logit']==0]\n",
    "    x = data_valid_test[data_valid_test['logit'] == 0][features_list]\n",
    "    data_valid_test[data_valid_test['logit'] == 0]['repost_hat'],   \\\n",
    "    data_valid_test[data_valid_test['logit'] == 0]['comments_hat'], \\\n",
    "    data_valid_test[data_valid_test['logit'] == 0]['likes_hat']     \\\n",
    "                                =                                   \\\n",
    "                                model_repost.predict(x),            \\\n",
    "                                model_comments.predict(x),          \\\n",
    "                                model_likes.predict(x)\n",
    "\n",
    "else:\n",
    "    x = data_valid_test[features_list]\n",
    "    data_valid_test['repost_hat'],   \\\n",
    "    data_valid_test['comments_hat'], \\\n",
    "    data_valid_test['likes_hat']     \\\n",
    "                                =                                   \\\n",
    "                                model_repost.predict(x),            \\\n",
    "                                model_comments.predict(x),          \\\n",
    "                                model_likes.predict(x)\n",
    "precision(data_valid_test[['repost', 'comments', 'likes', 'repost_hat', 'comments_hat', 'likes_hat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3003861595223833"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = data_valid_test[['user_id', 'repost', 'comments', 'likes', 'repost_hat', 'comments_hat', 'likes_hat']]\n",
    "\n",
    "m = data_train.groupby(['user_id'])[['repost', 'comments', 'likes']].median()\n",
    "m = m.sort_values(by='repost', ascending=False)\n",
    "m.columns = ['repost_hat', 'comments_hat', 'likes_hat']\n",
    "tmp = pd.merge(tmp, m, how='left', on='user_id')\n",
    "\n",
    "tmp['repost_hat_y'] = tmp['repost_hat_y'].fillna(tmp['repost_hat_x'])\n",
    "tmp['comments_hat_y'] = tmp['comments_hat_y'].fillna(tmp['comments_hat_x'])\n",
    "tmp['likes_hat_y'] = tmp['likes_hat_y'].fillna(tmp['likes_hat_x'])\n",
    "tmp['repost_hat'], tmp['comments_hat'] , tmp['likes_hat']  =  tmp['repost_hat_y'] ,tmp['comments_hat_y'],tmp['likes_hat_y'] \n",
    "precision(tmp[['repost', 'comments', 'likes', 'repost_hat', 'comments_hat', 'likes_hat']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7739.445409957983 129.67003050799613 757.1965567220733\n"
     ]
    }
   ],
   "source": [
    "#mse(data_valid['repost'], data_valid['repost_hat'].apply(lambda x: int(x*1.3)))\n",
    "print(mse(tmp['repost'], tmp['repost_hat']),\\\n",
    "         mse(tmp['comments'], tmp['comments_hat']), \\\n",
    "         mse(tmp['likes'], tmp['likes_hat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找最佳的整数转换系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.09, 0.31350886929008964]\n"
     ]
    }
   ],
   "source": [
    "int_tmp = tmp.loc[:, ['repost', 'comments', 'likes', 'repost_hat', 'comments_hat', 'likes_hat']]\n",
    "i = 1.0\n",
    "best = [0, 0]\n",
    "for j in range(1, 100):\n",
    "    for col in ['repost_hat', 'comments_hat', 'likes_hat']:\n",
    "        int_tmp.loc[:, col] = int_tmp.loc[:, col].apply(lambda x:int(i * x))\n",
    "    score = precision(int_tmp)\n",
    "    #print(i, score)\n",
    "    if score > best[1]:\n",
    "        best[0], best[1] = i, score\n",
    "    else:\n",
    "        pass\n",
    "    i += 0.01\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyrx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>weibo_id</th>\n",
       "      <th>repost_hat</th>\n",
       "      <th>comments_hat</th>\n",
       "      <th>likes_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fa5aed172c062c61e196eac61038a03b</td>\n",
       "      <td>7cce78a4ad39a91ec1f595bcc7fb5eba</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>77fc723c196a45203e70f4d359c96946</td>\n",
       "      <td>a3494d8cf475a92739a2ffd421640ddf</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>e4097b07f34366399b623b94f174f60c</td>\n",
       "      <td>6b89aea5aa7af093dde0894156c49dd3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>d43f7557c303b84070b13aa4eeeb21d3</td>\n",
       "      <td>0bdeff19392e15737775abab46dc5437</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>87465974e53e9f047e355e6e9b135b55</td>\n",
       "      <td>545c14094cbe50679daa63fe16419111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id                          weibo_id  \\\n",
       "0  fa5aed172c062c61e196eac61038a03b  7cce78a4ad39a91ec1f595bcc7fb5eba   \n",
       "1  77fc723c196a45203e70f4d359c96946  a3494d8cf475a92739a2ffd421640ddf   \n",
       "2  e4097b07f34366399b623b94f174f60c  6b89aea5aa7af093dde0894156c49dd3   \n",
       "3  d43f7557c303b84070b13aa4eeeb21d3  0bdeff19392e15737775abab46dc5437   \n",
       "4  87465974e53e9f047e355e6e9b135b55  545c14094cbe50679daa63fe16419111   \n",
       "\n",
       "   repost_hat  comments_hat  likes_hat  \n",
       "0           0             0          0  \n",
       "1           1             0          0  \n",
       "2           0             0          0  \n",
       "3           0             0          0  \n",
       "4           0             0          0  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取和定义数据\n",
    "data_predict=pd.read_csv('data_predict_tfidf.txt',index_col=[0],header=0)\n",
    "x = data_predict[features_list]\n",
    "\n",
    "data_predict['repost_hat'],         \\\n",
    "data_predict['comments_hat'],       \\\n",
    "data_predict['likes_hat']           \\\n",
    "        =                           \\\n",
    "        model_repost.predict(x),    \\\n",
    "        model_comments.predict(x),  \\\n",
    "        model_likes.predict(x)\n",
    "\n",
    "tmp = data_predict[['user_id', 'weibo_id', 'repost_hat', 'comments_hat', 'likes_hat']]\n",
    "m = data_train.groupby(['user_id'])[['repost', 'comments', 'likes']].median()\n",
    "m = m.sort_values(by='repost', ascending=False)\n",
    "m.columns = ['repost_hat', 'comments_hat', 'likes_hat']\n",
    "tmp = pd.merge(tmp, m, how='left', on='user_id')\n",
    "\n",
    "tmp['repost_hat_y']   = tmp['repost_hat_y'].fillna(tmp['repost_hat_x'])\n",
    "tmp['comments_hat_y'] = tmp['comments_hat_y'].fillna(tmp['comments_hat_x'])\n",
    "tmp['likes_hat_y']    = tmp['likes_hat_y'].fillna(tmp['likes_hat_x'])\n",
    "tmp['repost_hat'], tmp['comments_hat'] , tmp['likes_hat']  = tmp['repost_hat_y'] ,tmp['comments_hat_y'],tmp['likes_hat_y'] \n",
    "\n",
    "if noise_cancellation == True:\n",
    "    tmp[tmp['logit'] == 0]['repost_hat'], tmp[tmp['logit'] == 0]['comments_hat'] , tmp[tmp['logit'] == 0]['likes_hat']  = 0, 0, 0\n",
    "\n",
    "result = tmp[['user_id', 'weibo_id', 'repost_hat', 'comments_hat', 'likes_hat']]\n",
    "\n",
    "for col in ['repost_hat', 'comments_hat', 'likes_hat']:\n",
    "    result[col] = result[col].apply(lambda x:int(1.3 * x)) # best mutiplier\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 按照线上提交要求重组并存储数据\n",
    "result = result.assign(\n",
    "    predict=result.repost_hat.astype(str) + ',' +\n",
    "    result.comments_hat.astype(str)       + ',' +\n",
    "    result.likes_hat.astype(str))         # longer but faster than apply lambda\n",
    "result = result[['user_id', 'weibo_id', 'predict']]\n",
    "result.columns = ['uid', 'mid', 'predict']\n",
    "result.to_csv(\"./result.txt\", header=False, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE MODEL TO DISK\n",
    "joblib.dump(model_repost, './model/model_repost.sav')\n",
    "joblib.dump(model_repost, './model/model_comments.sav')\n",
    "joblib.dump(model_repost, './model/model_likes.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
